{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Transfer Learning for Deep Networks\n",
    "\n",
    "## Introduction\n",
    "The model constructed in this notebook tries to dynamically adapt its weights when confronted with new tasks. A method named **elastic weight consolidation (EWC)** ([Kirkpatrick, 2016](http://www.pnas.org/content/114/13/3521.full.pdf)) is implemented that considers data from two different tasks as independent. By inferring the posterior weight distribution in task **A** $p(w|D_A)$, a model is trained which is able to solve the second task **B** when exposed to new data $D_B$, while remembering task **A**. Variational Bayasian approximations of $p(w|D_A)$ are considered for this operation.\n",
    "\n",
    "comment: shall we write an introduction? If yes, shall we also mention normalizing flows here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from auxiliary import merge_add, merge_average\n",
    "from bbbmlp import BBBMLP\n",
    "from convbbbmlp import BBBCNN\n",
    "from datasets import LimitedMNIST\n",
    "from loggers import PrintLogger, WeightLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cuda        = torch.cuda.is_available()\n",
    "NUM_EPOCHS  = 50\n",
    "SAVE_EVERY  = 9\n",
    "N_SAMPLES   = 1\n",
    "LR          = 1e-3\n",
    "MNIST       = \"./\"\n",
    "number_of_flows = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_logvar_init = 0.\n",
    "q_logvar_init = -5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### call log functions for weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_logger = WeightLogger()\n",
    "print_logger = PrintLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define network\n",
    "\n",
    "Bayes by Backprop Multi Layer Perceptron (BBBMLP) with 2 hidden layers, each with 100 units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(filename, digits=[0], fraction=1.0, pretrained=False):\n",
    "    mnist_train = LimitedMNIST(root=MNIST, set_type=\"train\", transform=lambda x: x.reshape(-1, 28, 28),\n",
    "                               target_transform=lambda x: x - min(digits),\n",
    "                               digits=digits, fraction=fraction)\n",
    "\n",
    "    mnist_val = LimitedMNIST(root=MNIST, set_type=\"validation\", transform=lambda x: x.reshape(-1, 28, 28),\n",
    "                             target_transform=lambda x: x - min(digits),\n",
    "                             digits=digits, fraction=fraction)\n",
    "\n",
    "    batch_size = 64\n",
    "    loader_train = DataLoader(mnist_train, batch_size=batch_size, num_workers=2, pin_memory=cuda)\n",
    "    loader_val = DataLoader(mnist_val, batch_size=batch_size, num_workers=2, pin_memory=cuda)\n",
    "\n",
    "    model = BBBMLP(in_features=784, num_class=len(digits), num_hidden=100, num_layers=2,\n",
    "                   p_logvar_init=p_logvar_init, p_pi=1.0, q_logvar_init=q_logvar_init, nflows=number_of_flows)\n",
    "\n",
    "    if pretrained:\n",
    "        path = \"original/weights/model_epoch49.pkl\"\n",
    "        d = pickle.load(open(path, \"rb\"))\n",
    "        model.load_state_dict(d)\n",
    "\n",
    "    file_logger.initialise(filename)\n",
    "    print_logger.initialise(filename)\n",
    "\n",
    "    # Create optimizer\n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=LR)\n",
    "\n",
    "    if cuda:\n",
    "        model.cuda()\n",
    "\n",
    "    def run_epoch(loader, MAP=False, is_training=False):\n",
    "        diagnostics = {}\n",
    "        nbatch_per_epoch = len(loader.dataset) // loader.batch_size\n",
    "\n",
    "        for i, (data, labels) in tqdm(enumerate(loader)):\n",
    "            # Repeat samples\n",
    "            x = data.repeat(N_SAMPLES, 1, 1, 1)\n",
    "            y = labels.repeat(N_SAMPLES, 0)\n",
    "            x = x.view(-1, 784)\n",
    "\n",
    "            if cuda:\n",
    "                x = x.cuda()\n",
    "                y = y.cuda()\n",
    "\n",
    "            logits, loss, _diagnostics = model.getloss(Variable(x), Variable(y),\n",
    "                                                       dataset_size=len(loader.dataset), MAP=MAP)\n",
    "            diagnostics = merge_add(diagnostics, _diagnostics)\n",
    "\n",
    "            if is_training:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        diagnostics = merge_average(diagnostics, nbatch_per_epoch)\n",
    "        return diagnostics\n",
    "\n",
    "\n",
    "    diagnostics_batch_train, diagnostics_batch_valid, diagnostics_batch_valid_MAP = [], [], []\n",
    "\n",
    "    file_logger.dump(model, -1, None, p_logvar_init)\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        diagnostics_batch_train += [run_epoch(loader_train, is_training=True)]\n",
    "        diagnostics_batch_valid += [run_epoch(loader_val)]\n",
    "        diagnostics_batch_valid_MAP += [run_epoch(loader_val, MAP=True)]\n",
    "\n",
    "        batch_diagnostics = [diagnostics_batch_train, diagnostics_batch_valid, diagnostics_batch_valid_MAP]\n",
    "\n",
    "        if epoch % SAVE_EVERY == 0:\n",
    "            file_logger.dump(model, epoch, batch_diagnostics, p_logvar_init)\n",
    "\n",
    "        print_logger.dump(epoch, batch_diagnostics)\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "    file_logger.dump(model, epoch, batch_diagnostics, p_logvar_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters for experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "digits = [0, 1, 2, 3, 4]\n",
    "transfer = [5, 6, 7, 8, 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Call model to train it first on the data of 'digits'\n",
    "\n",
    "To get distribution $p(w | D_A)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_model(\"original\", digits, fraction=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Call model to train on the data of 'transfer'\n",
    "\n",
    "To get distribution $p(w|D_B)$ when trained with a **uniform prior**.\n",
    "\n",
    "Different fractions for comparison of performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_model(\"domain0.05\", transfer, fraction=0.05, pretrained=False)\n",
    "train_model(\"domain0.1\", transfer, fraction=0.1, pretrained=False)\n",
    "train_model(\"domain0.2\", transfer, fraction=0.2, pretrained=False)\n",
    "train_model(\"domain0.3\", transfer, fraction=0.3, pretrained=False)\n",
    "train_model(\"domain0.5\", transfer, fraction=0.5, pretrained=False)\n",
    "train_model(\"domain1\", transfer, fraction=1.0, pretrained=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Transfer to the second domain with the trained model\n",
    "\n",
    "To get distribution $p(w|D_B)$ when trained with $p(w|D_A)$ as its **pretrained prior**.\n",
    "\n",
    "Different fractions for comparison of performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_model(\"transfer_domain0.05\", transfer, fraction=0.05, pretrained=True)\n",
    "train_model(\"transfer_domain0.1\", transfer, fraction=0.1, pretrained=True)\n",
    "train_model(\"transfer_domain0.2\", transfer, fraction=0.2, pretrained=True)\n",
    "train_model(\"transfer_domain0.3\", transfer, fraction=0.3, pretrained=True)\n",
    "train_model(\"transfer_domain0.5\", transfer, fraction=0.5, pretrained=True)\n",
    "train_model(\"transfer_domain1\", transfer, fraction=1.0, pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"seaborn\")\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def load_data(basename, intervals):\n",
    "    files = [open(\"{}{}/logfile.txt\".format(basename, i)).read() for i in intervals]\n",
    "    acc = [list(map(lambda x: x.split(\" \")[-1], re.findall(r\"(acc: \\d.\\d+)\", f))) for f in files]\n",
    "    if basename is \"domain\":\n",
    "        print(acc)\n",
    "    train = list(map(lambda x: x[-3], acc))\n",
    "    valid = list(map(lambda x: x[-2], acc))\n",
    "    MAP = list(map(lambda x: x[-1], acc))\n",
    "    return np.array(train).astype(np.float32), np.array(valid).astype(np.float32), np.array(MAP).astype(np.float32)\n",
    "\n",
    "i = [0.05, 0.1, 0.2, 0.3, 0.5, 1]\n",
    "f = plt.figure(figsize=(10, 8))\n",
    "\n",
    "train, valid, MAP = load_data(\"transfer_cnn_domain\", [0.05, 0.1, 0.2, 0.3, 0.5, 0.5])\n",
    "\n",
    "plt.plot(i, train, label=r\"Train, prior: $q(w \\mid \\theta)$\", color=\"#9c209b\")\n",
    "plt.plot(i, valid, \"--\", label=r\"Validation, prior: $q(w \\mid \\theta)$\", color=\"#d534d3\")\n",
    "plt.plot(i, MAP, \"--\", label=r\"MAP, prior: $q(w \\mid \\theta)$\", color=\"#e273e1\")\n",
    "\n",
    "train, valid, MAP = load_data(\"cnn_domain\", i)\n",
    "\n",
    "plt.plot(i, train, label=r\"Train, prior: $\\mathcal{U}(a, b)$\", color=\"#209c22\")\n",
    "plt.plot(i, valid, \"--\", label=r\"Validation, prior: $\\mathcal{U}(a, b)$\", color=\"#34d536\")\n",
    "plt.plot(i, MAP, \"--\", label=r\"MAP, prior: $\\mathcal{U}(a, b)$\", color=\"#73e275\")\n",
    "\n",
    "plt.xlabel(\"Size of transfer dataset\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xticks(i, map(lambda x: \"{}%\".format(int(x*100)), i))\n",
    "f.suptitle(\"Accuracy after training for 50 epochs\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.savefig(\"cnn_train_acc.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
