{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Transfer Learning for Deep Networks\n",
    "\n",
    "In this project we are concerned with **Bayesian Deep Learning**. Specifically, we want to know whether having a deep Bayesian model will improve the transfer of learning. Our hypothesis is that that knowledge gained from training a model on tasks **A** and then using the learned weights as a basis for learning on tasks $B$ will perform better than training **B** from scratch - assuming the domains are similar.\n",
    "\n",
    "![Transfer Learning](https://image.slidesharecdn.com/13aibigdata-160606103446/95/aibigdata-lab-2016-transfer-learning-7-638.jpg?cb=1465209397)\n",
    "\n",
    "We use Bayes By Backprop introduced by [Blundell, 2015](https://arxiv.org/abs/1505.05424)). to learn a probability distribution over each of the weights in the network. These weight distributions are fitted using variational inference given some prior.\n",
    "\n",
    "By inferring the posterior weight distribution in task **A** $p(w|D_A)$, a model is trained which is able to solve the second task **B** when exposed to new data $D_B$, while remembering task **A**. Variational Bayasian approximations of $p(w|D_A)$ are considered for this operation.\n",
    "\n",
    "> The model constructed in this notebook tries to dynamically adapt its weights when confronted with new tasks. A method named **elastic weight consolidation (EWC)** ([Kirkpatrick, 2016](http://www.pnas.org/content/114/13/3521.full.pdf)) is implemented that considers data from two different tasks as independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and declarations\n",
    "import torch\n",
    "import sys\n",
    "import numpy as np\n",
    "sys.path.append(\"bayesian_transfer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda        = torch.cuda.is_available()\n",
    "num_epochs  = 250\n",
    "num_samples = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning\n",
    "\n",
    "We quickly show how to get transfer learning up and running in this Bayesian setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayesian_transfer import BBBMLP\n",
    "from experiment import get_data\n",
    "\n",
    "def get_model(num_output, num_hidden=100, num_layers=2, num_flows=0, pretrained=None):\n",
    "    model = BBBMLP(in_features=784, num_class=num_output, num_hidden=num_hidden, num_layers=num_layers, nflows=num_flows, p_logvar_init = -2)\n",
    "\n",
    "    if pretrained:\n",
    "        d = pretrained.state_dict()\n",
    "        model.load_prior(d)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop. Here we are forward declaring the loss function\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "def run_epoch(model, loader, epoch, is_training=False):\n",
    "    # Number of mini.batches\n",
    "    m = math.ceil(len(loader.dataset) / loader.batch_size)\n",
    "    \n",
    "    diagnostics = {\"accuracy\": [], \"likelihood\": [],\n",
    "                   \"KL\": [], \"loss\": []}\n",
    "\n",
    "    for i, (data, labels) in enumerate(tqdm(loader)):\n",
    "        # Repeat samples\n",
    "        x = data.view(-1, 784).repeat(num_samples, 1)\n",
    "        y = labels.repeat(num_samples)\n",
    "\n",
    "        if cuda:\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "        # Blundell Beta-scheme\n",
    "        beta = 2 ** (m - (i + 1)) / (2 ** m - 1)\n",
    "\n",
    "        # Calculate loss\n",
    "        logits, kl = model.probforward(Variable(x))\n",
    "        loss = loss_fn(logits, Variable(y), kl, beta)\n",
    "        ll = -loss.data.mean() + beta*kl.data.mean()\n",
    "\n",
    "        # Update gradients\n",
    "        if is_training:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Compute accuracy\n",
    "        _, predicted = logits.max(1)\n",
    "        accuracy = (predicted.data == y).float().mean()\n",
    "\n",
    "        diagnostics[\"accuracy\"].append(accuracy/m)\n",
    "        diagnostics[\"loss\"].append(loss.data.mean()/m)\n",
    "        diagnostics[\"KL\"].append(beta*kl.data.mean()/m)\n",
    "        diagnostics[\"likelihood\"].append(ll/m)\n",
    "\n",
    "    return diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model on A\n",
    "\n",
    "For this, we limit the dataset to only the first 5 digits of MNIST and use all of the data within this subset. We use a standard 2-layer Bayesian NN with 400 hidden units. We are using the following loss function\n",
    "\n",
    "$$\\theta^* = \\text{arg}\\min_{\\theta} = KL(q(w|\\theta)||p(w)) - \\mathbb{E}_{q(w|\\theta)}[\\ln p(\\mathcal{D}_A|w)]$$\n",
    "\n",
    "where $ p(\\mathcal{D_A}|w)$ is a suitable likelihood function, such as cross-entropy - for the data in the first domain $\\mathcal{D}_A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayesian_transfer import GaussianVariationalInference\n",
    "from torch.autograd import Variable\n",
    "\n",
    "digits = [0, 1, 2, 3, 4]\n",
    "loader_train, loader_val = get_data(digits, fraction=1.0)\n",
    "\n",
    "model_a = get_model(5, num_hidden=400, num_layers=2, num_flows=0)\n",
    "\n",
    "# Define the objective, in this case we want to minimize the negative free free energy.\n",
    "loss_fn = GaussianVariationalInference(torch.nn.CrossEntropyLoss())\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model_a.parameters()), lr=1e-3)\n",
    "\n",
    "if cuda: model_a.cuda()\n",
    "    \n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch {}/{}\".format(epoch, num_epochs))\n",
    "    diagnostics_train = run_epoch(model_a, loader_train, epoch, is_training=True)\n",
    "    diagnostics_val = run_epoch(model_a, loader_val, epoch)\n",
    "\n",
    "    diagnostics_train = dict({\"type\": \"train\", \"epoch\": epoch}, **diagnostics_train)\n",
    "    diagnostics_val = dict({\"type\": \"validation\", \"epoch\": epoch}, **diagnostics_val)\n",
    "\n",
    "    # Save model and diagnostics\n",
    "    print(diagnostics_train)\n",
    "    print(diagnostics_val)\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we transfer to the second domain, by using the same loss function, but instead, the likelihood is defined over the second domain $\\mathcal{D}_B$. We also use the learned posterior from the first part as a prior for transfer. Consequently, the loss function is now defined as:\n",
    "\n",
    "$$\\theta^* = \\text{arg}\\min_{\\theta} = KL(q(w|\\theta)||q_A(w)) - \\mathbb{E}_{q(w|\\theta)}[\\ln p(\\mathcal{D}_B|w)]$$\n",
    "\n",
    "where $q_A$ is learned from the first domain. Implementation-wise we only need to include the trained model as a prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = [5, 6, 7, 8, 9]\n",
    "loader_train, loader_val = get_data(digits, fraction=1.0)\n",
    "\n",
    "\n",
    "model_b = get_model(5, num_hidden=400, num_layers=2, num_flows=0, pretrained=model_a)\n",
    "\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model_b.parameters()), lr=1e-3)\n",
    "\n",
    "if cuda: model_b.cuda()\n",
    "    \n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch {}/{}\".format(epoch, num_epochs))\n",
    "    diagnostics_train = run_epoch(model_b, loader_train, epoch, is_training=True)\n",
    "    diagnostics_val = run_epoch(model_b, loader_val, epoch)\n",
    "\n",
    "    diagnostics_train = dict({\"type\": \"train\", \"epoch\": epoch}, **diagnostics_train)\n",
    "    diagnostics_val = dict({\"type\": \"validation\", \"epoch\": epoch}, **diagnostics_val)\n",
    "\n",
    "    # Save model and diagnostics\n",
    "    print(diagnostics_train)\n",
    "    print(diagnostics_val)\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Plotting the results, we see that transfer learning does not work particularly well for this domain.\n",
    "\n",
    "![](figs/transfer_results.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch35)",
   "language": "python",
   "name": "torch35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
